{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 - Variational Autoencoder for Deep Generative Learning\n",
    "\n",
    "## Objective\n",
    "\n",
    "* To build a variational autoencoder that can generate images from the learned model \n",
    "\n",
    "**Suggested reading**: \n",
    "* [Variational Bayesian methods on Wiki](https://en.wikipedia.org/wiki/Variational_Bayesian_methods)\n",
    "* [Tutorial on Variational Autoencoders](https://arxiv.org/pdf/1606.05908.pdf)\n",
    "* [Variational autoencoders notes](https://deepgenerativemodels.github.io/notes/vae/)\n",
    "\n",
    "This notebook is based on the [Tutorial - What is a variational autoencoder?](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/) by Jaan Altosaar, PyTorch's [Variational Autoencoder code](https://github.com/pytorch/examples/blob/master/vae/main.py), [Martin Krasser](https://github.com/krasserm)'s [Stochastic variational inference and variational autoencoders](https://nbviewer.jupyter.org/github/krasserm/bayesian-machine-learning/blob/dev/latent-variable-models/latent_variable_models_part_2.ipynb) notebook, [Pyro](http://pyro.ai/)'s [Variational Autoencoders tutorial](http://pyro.ai/examples/vae.html), and [Nitarshan](https://github.com/nitarshan)'s [Variational Autoencoders](https://github.com/nitarshan/variational-autoencoder/blob/master/Variational%20Autoencoder%20Tutorial.ipynb) notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why\n",
    "\n",
    "**Deep generative models** aim to combine the interpretable representations and quantified uncertainty offered by probabilistic models, with the flexibility and scalable learning of deep neural networks, from the [Deep Generative Models](http://stat.columbia.edu/~cunningham/teaching/GR8201/) course by [John P. Cunningham](http://stat.columbia.edu/~cunningham/). \n",
    "\n",
    "**Variational autoencoders** are such deep generative models. They let us design complex generative models of data, and fit them to large datasets. They yield state-of-the-art machine learning results in image generation and reinforcement learning and can generate [images of fictional celebrity faces](https://www.youtube.com/watch?v=XNZIN7Jh3Sg) and [high-resolution digital artwork](https://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (VAE)\n",
    "Like all autoencoders, the variational autoencoder is primarily used for unsupervised learning of hidden/latent representations. However, they are fundamentally different in that they approach the problem from a probabilistic perspective. They specify a joint distribution over the observed and latent variables, and approximate the intractable posterior conditional density over latent variables with [variational inference](https://en.wikipedia.org/wiki/Variational_Bayesian_methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network architecture\n",
    "A variational autoencoder consists of an encoder, a decoder, and a loss function as a typical autoencoder: \n",
    "\n",
    "<img src=\"https://jaan.io/images/encoder-decoder.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height: 200px\" />\n",
    "     \n",
    "* The **encoder** takes a datapoint $x$ as the input and encodes it to produce a hidden/latent representation $z$ as the output, with parameters $\\theta$ (weights and biases). The encoder in a VAE can be written as a Gaussian probability density $q_\\theta (z \\mid x)$ to model a stochastic lower-dimensional space is stochastic. The encoder outputs parameters to $q_\\theta (z \\mid x)$ and we can sample from this distribution to get noisy values of the representations $z$. \n",
    "* The **decoder** $p_\\phi(x\\mid z)$ takes the representation $z$ as the input and outputs the parameters to the probability distribution of the data $p_\\phi (x\\mid z)$, with parameters $\\phi$ (weights and biases). The reconstruction log-likelihood $\\log p_\\phi (x\\mid z)$ tells us how effectively the decoder has learned to reconstruct (autoencode) an input image $x$ given its latent representation $z$.\n",
    "\n",
    "* <p>The **loss function** is the negative log-likelihood with a regularizer. The loss function $l_i$ for a single datapoint $x_i$ is:\n",
    "\\begin{equation}\n",
    "\\:\\\\\n",
    "l_i(\\theta, \\phi) = - \\mathbb{E}_{z\\sim q_\\theta(z\\mid x_i)}[\\log p_\\phi(x_i\\mid z)] + \\mathbb{KL}(q_\\theta(z\\mid x_i) \\mid\\mid p(z))\\\\\n",
    "\\tag{1}\\end{equation}\n",
    "Datapoints are assumed to be independent so the total loss is $\\sum_{i=1}^N l_i$ for $N$ total datapoints. The first term aims for reconstruction and is defined as the expected negative log-likelihood of the $i$-th datapoint, with the expectation taken w.r.t. to the encoder’s distribution over the representations. The second term is a regularizer in the form of the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the encoder’s\n",
    "distribution $q_\\theta(z\\mid x)$ (the approximation) and $p(z)$ (the ground truth), measuring their mismatch.\n",
    " \n",
    "The variational autoencoder specifies $p$ as a standard Gaussian distribution with mean zero and variance one. If the encoder outputs representations $z$ having a distribution different from a standard Gaussian, there will be a penalty (proportional to the amount of difference) in the loss. The training of VAE uses gradient descent to optimize the loss with respect to the parameters of the encoder and decoder $\\theta$ and $\\phi$. \n",
    "\n",
    "**Example**: A $28\\times 28$ handwritten digit image can be represented as $0$ or $1$ (black or white). We use a Bernoulli distribution to represent the probability distribution of a single pixel. The decoder receives the latent representation of a digit $z$ and outputs $784$ Bernoulli parameters, one for each of the $784$ pixels, from which we can **sample** or **generate** digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic modelling in variational autoencoder \n",
    "\n",
    "*This section is **optional**. You may skip it safely if you are only interested in using VAE.*\n",
    "\n",
    "From a probability model perspective, a variational autoencoder is a latent variable model that **generates** a datapoint $x$ from latent variables $z$ with a joint probability $p(x, z) = p(x \\mid z) p(z)$. \n",
    "<img src=\"https://jaan.io/images/graphical-model-variational-autoencoder.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height: 200px\" />\n",
    "For each datapoint $i$, latent variable $z_i$ are first drawn from a prior distribution $p(z)$: $z_i \\sim p(z)$. Then, a datapoint $x_i$ is drawn from the likelihood $p(x\\mid z)$: $x_i \\sim p(x\\mid z)$. Recall that we have $p(x,z) = p(x\\mid z)p(z)$. For a black and white digit image, the likelihood has a Bernoulli distribution.\n",
    "\n",
    "With a Bayesian approach, the **inference** task is to learn good values of the latent variables given observed data in terms of the posterior\n",
    "\n",
    "$$p(z \\mid x) = \\frac{p(x \\mid z)p(z)}{p(x)}.$$\n",
    "\n",
    "The denominator $p(x)$ is the marginal distribution of $x$ (the evidence): $p(x) = \\int p(x \\mid z) p(z) dz$. This integral requires exponential time to compute so we need to an approximation.  \n",
    "\n",
    "**Variational inference** approximates the posterior with a family of distributions $q_\\lambda(z \\mid x)$ indexed by the variational parameter(s) $\\lambda$, which are the mean and variance for Gaussian distributions: $\\lambda = (\\mu, \\sigma^2))$. We can use the **Kullback-Leibler divergence** to measure how well the variational posterior $q(z \\mid x)$ approximates the true posterior $p(z \\mid x)$: \n",
    "\n",
    "$$\\mathbb{KL}(q_\\lambda(z \\mid x) \\mid \\mid p(z \\mid x)) = \\mathbf{E}_q[\\log q_\\lambda(z \\mid x)]- \\mathbf{E}_q[\\log p(x, z)] + \\log p(x)$$\n",
    "\n",
    "Optimizing the KL divergence above is intractable. However, if we define the **Evidence Lower BOund** (ELBO)\n",
    "\n",
    "$$ELBO(\\lambda) = \\mathbf{E}_q[\\log p(x, z)] - \\mathbf{E}_q[\\log q_\\lambda(z \\mid x)],$$\n",
    "\n",
    "then we can combine this with the KL divergence and rewrite the evidence (marginal distribution) as</p>\n",
    "\n",
    "$$\\log p(x) = ELBO(\\lambda) + \\mathbb{KL}(q_\\lambda(z \\mid x) \\mid \\mid p(z \\mid x))$$\n",
    "\n",
    "By Jensen’s inequality, the KL divergence is always non-negative so minimizing this KL divergence is equivalent to maximizing the ELBO because $p(x)$ is fixed (though intractable). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 1\n",
    "\n",
    "Why is $ELBO(\\lambda)$ the lower bound for $\\log p(x)$?\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ELBO enables approximate posterior inference.Maximizing the ELBO is equivalent (but computationally tractable) to minimizing the KL divergence between the approximate and exact posteriors.\n",
    "\n",
    "The variational autoencoder model further assumes that no datapoint shares its latent $z$ with another datapoint so that we can decompose the ELBO as a sum of the ELBOs for each single datapoint:\n",
    "\n",
    "$$ELBO_i(\\lambda) = \\mathbb{E}_{q_\\lambda(z\\mid x_i)}[\\log p(x_i\\mid z)] - \\mathbb{\\mathbb{KL}}(q_\\lambda(z\\mid x_i) \\mid\\mid p(z)).$$\n",
    "\n",
    "Now, we can use stochastic gradient descent with respect to the parameters $\\lambda$ (which are shared across datapoints)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 2\n",
    "\n",
    "Verify that this definition for $ELBO_i(\\lambda)$ is equivalent to our previous definition of the ELBO. \n",
    "\n",
    "*Hint: expand the log joint into the prior and likelihood terms and use the product rule for the logarithm.*\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we parametrize the approximate posterior $q_\\theta (z \\mid x, \\lambda)$ with an **inference network** (encoder) that takes data $x$ in and outputs parameters $\\lambda$, with learnable parameter $\\theta$ (weights and biases). And we parameterize the likelihood $p(x \\mid z)$ with a **generative network** (decoder) that takes latent variables in and outputs parameters to the data distribution $p_\\phi(x \\mid z)$, with learnable parameters $\\phi$ (weights and biases). $\\theta$ and $\\phi$ can be learned by maximizing the ELBO using stochastic gradient descent with minibatch. We can write the ELBO and include the inference and generative network parameters as:</p>\n",
    "\n",
    "$$ELBO_i(\\theta, \\phi) = \\mathbb{E}_{q_\\theta(z\\mid x_i)}[\\log p_\\phi(x_i\\mid z)] - \\mathbb{KL}(q_\\theta(z\\mid x_i) \\mid\\mid p(z)).$$\n",
    "\n",
    "Notice that this evidence lower bound is the negative of the loss function $l_i(\\theta, \\phi)$ defined in Equation (1): $ELBO_i(\\theta, \\phi) = -l_i(\\theta, \\phi)$. \n",
    "\n",
    "The probability model approach reveals that two terms in Equation (1) aim to minimize the KL divergence (mismatch) between the approximate posterior $q_\\lambda(z \\mid x)$ and (true) model posterior $p(z \\mid x)$.\n",
    "\n",
    "**Summary**: In this variational inference, we've defined a probability model $p$ of latent variables and data, as well as a variational family $q$ for the latent variables to approximate our posterior. Then the variational inference algorithm can learn the variational parameters through gradient *ascent* on the ELBO. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reparametrization trick\n",
    "\n",
    "To implement the variational autoencoder, we need to take derivatives w.r.t. the parameters of a stochastic variable. If $z$  is drawn from a distribution $q_\\theta (z \\mid x)$, how to take derivatives of a function of $z$ with respect to $\\theta$? The  sample $z$ is fixed, but intuitively its derivative should be nonzero.\n",
    "\n",
    "For Gaussian distributions, we can reparametrize samples in a clever way so that the stochasticity is independent of the parameters, e.g. by making samples to deterministically depend on the parameters of the distribution. For example, in a Gaussian variable with mean $\\mu$ and standard devation $\\sigma$, we can sample from it as:</p>\n",
    "\n",
    "$$z = \\mu + \\sigma \\odot \\epsilon,$$\n",
    "\n",
    "<p>where $\\epsilon \\sim \\mathcal{N}(0, 1)$. In this way, we have defined a function which depends on the parameters deterministically so that we can take derivatives of functions involving $z$, $f(z)$ w.r.t. to the parameters of its distribution $\\mu$ and $\\sigma$, which are output by an inference network with parameters $\\theta$ that we optimize. \n",
    "\n",
    "<img src=\"https://jaan.io/images/reparametrization.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height: 200px\" />\n",
    "\n",
    "Through this trick, we can backpropagate w.r.t. $\\theta$ through the objective (the ELBO) which is a function of samples of the latent variables $z$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoder via PyTorch\n",
    "\n",
    "Let us implement a VAE in PyTorch to learn a generative model for handwritten digit images from the  MNIST dataset. \n",
    "\n",
    "First, get ready by importing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from tqdm import tnrange\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "torch.manual_seed(2020)\n",
    "sns.set_style('dark')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE MNIST Example\n",
    "batch_size=128 # input batch size for training (default: 128)\n",
    "Max_epochs=10 # number of epochs to train (default: 10)')\n",
    "log_interval=200 # how many batches to wait before logging training status\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Use GPU if you have one\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}\n",
    "\n",
    "train_loader = DataLoader(datasets.MNIST('data', train=True, download=True, \n",
    "        transform=transforms.ToTensor()), batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(datasets.MNIST('data', train=False, \n",
    "        transform=transforms.ToTensor()), batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct the VAE model and do an inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (fc1): Linear(in_features=784, out_features=400, bias=True)\n",
      "  (fc21): Linear(in_features=400, out_features=20, bias=True)\n",
      "  (fc22): Linear(in_features=400, out_features=20, bias=True)\n",
      "  (fc3): Linear(in_features=20, out_features=400, bias=True)\n",
      "  (fc4): Linear(in_features=400, out_features=784, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self,latent_dim=20,hidden_dim=400):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim) # mean\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim) #logvar\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "myVAE = VAE().to(device)\n",
    "print(myVAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function and choose the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(myVAE.parameters(), lr=1e-3)\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the per-epoch training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    myVAE.train()\n",
    "    losses = [] # train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = myVAE(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item()) # train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the per-epoch test process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    myVAE.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = myVAE(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the actual training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 547.963135\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 148.517883\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 133.526489\n",
      "====> Epoch: 1 Average loss: 163.7404\n",
      "====> Test set loss: 126.4516\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 126.005630\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 124.283485\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 113.678886\n",
      "====> Epoch: 2 Average loss: 121.0321\n",
      "====> Test set loss: 115.5248\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 112.454666\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 115.604416\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 110.173447\n",
      "====> Epoch: 3 Average loss: 114.4379\n",
      "====> Test set loss: 111.8404\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 115.158539\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 110.021759\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 110.875534\n",
      "====> Epoch: 4 Average loss: 111.5716\n",
      "====> Test set loss: 109.7790\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 113.719872\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 109.595665\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 107.076973\n",
      "====> Epoch: 5 Average loss: 109.8866\n",
      "====> Test set loss: 108.5925\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 109.660904\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 108.067078\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 103.397171\n",
      "====> Epoch: 6 Average loss: 108.7726\n",
      "====> Test set loss: 107.6448\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 109.136322\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 106.912025\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 107.062187\n",
      "====> Epoch: 7 Average loss: 107.9123\n",
      "====> Test set loss: 107.1523\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 107.954185\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 108.070686\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 111.013763\n",
      "====> Epoch: 8 Average loss: 107.2864\n",
      "====> Test set loss: 106.7127\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 109.230583\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 106.684273\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 106.428993\n",
      "====> Epoch: 9 Average loss: 106.7973\n",
      "====> Test set loss: 106.1275\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 108.715599\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 106.879013\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 109.635368\n",
      "====> Epoch: 10 Average loss: 106.3198\n",
      "====> Test set loss: 105.8612\n"
     ]
    }
   ],
   "source": [
    "    for epoch in range(1, Max_epochs + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "            sample = myVAE.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercises\n",
    "\n",
    "\n",
    "* Explore other VAEs following the [PyTorch-VAE](https://github.com/AntixK/PyTorch-VAE) repository to generate new faces from celebrities.\n",
    "* Study [A Tutorial on Variational Autoencoders with a Concise Keras Implementation](https://tiao.io/post/tutorial-on-variational-autoencoders-with-a-concise-keras-implementation/) by Louis Tiao and implement the same in pytorch.\n",
    "* Explore [PyroLab 1 - Bayesian Linear Regression for Generative Learning with Pyro](https://github.com/haipinglu/SimplyDeep/blob/master/PyroLab%201%20-%20Bayesian%20Linear%20Regression%20for%20Generative%20Learning%20with%20Pyro.ipynb)\n",
    "* Explore [PyroLab 2 - Variational Autoencoder for Deep Generative Learning](https://github.com/haipinglu/SimplyDeep/blob/master/PyroLab%202%20-%20Variational%20Autoencoder%20for%20Deep%20Generative%20Learning.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
