{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 - Variational Autoencoder for Deep Generative Learning\n",
    "\n",
    "## Objective\n",
    "\n",
    "* To build a variational autoencoder that can generate images from the learned model \n",
    "\n",
    "**Suggested reading**: \n",
    "* [Variational Bayesian methods on Wiki](https://en.wikipedia.org/wiki/Variational_Bayesian_methods)\n",
    "* [Tutorial on Variational Autoencoders](https://arxiv.org/pdf/1606.05908.pdf)\n",
    "* [Variational autoencoders notes](https://deepgenerativemodels.github.io/notes/vae/)\n",
    "\n",
    "This notebook is based on PyTorch's [Variational Autoencoder code](https://github.com/pytorch/examples/blob/master/vae/main.py), [Martin Krasser](https://github.com/krasserm)'s [Stochastic variational inference and variational autoencoders](https://nbviewer.jupyter.org/github/krasserm/bayesian-machine-learning/blob/dev/latent-variable-models/latent_variable_models_part_2.ipynb) notebook, [Pyro](http://pyro.ai/)'s [Variational Autoencoders tutorial](http://pyro.ai/examples/vae.html), and [Nitarshan](https://github.com/nitarshan)'s [Variational Autoencoders](https://github.com/nitarshan/variational-autoencoder/blob/master/Variational%20Autoencoder%20Tutorial.ipynb) notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why\n",
    "\n",
    "**Deep generative models** aim to combine the interpretable representations and quantified uncertainty offered by probabilistic models, with the flexibility and scalable learning of deep neural networks. **Variational autoencoder** is such a model, one of the most exciting and rapidly evolving fields of statistical machine learning. This is the best description that I can find from the [Deep Generative Models](http://stat.columbia.edu/~cunningham/teaching/GR8201/) course by [John P. Cunningham](http://stat.columbia.edu/~cunningham/), Columbia University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Stochastic variational inference and variational autoencoders\n",
    "\n",
    "#### Latent variable models\n",
    "This section studies [latent variable models](https://en.wikipedia.org/wiki/Latent_variable_model) with continuous latent variables for modeling complex data such as natural images and the [variational Bayesian methods](https://en.wikipedia.org/wiki/Variational_Bayesian_methods) for inference with stochastic optimization algorithms.\n",
    "\n",
    "Now we consider a model with one continuous latent variable $\\mathbf{t}_i$ per observation $\\mathbf{x}_i$. We want to describe observations, e.g. images, $\\mathbf{X} = \\left\\{ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\right\\}$ with a probabilistic model $p(\\mathbf{x} \\lvert \\boldsymbol{\\theta})$. We aim to maximize the data likelihood $p(\\mathbf{X} \\lvert \\boldsymbol{\\theta})$ w.r.t. $\\boldsymbol{\\theta}$ and to obtain approximate posterior distributions over continuous latent variables. The joint distribution over an observed variable $\\mathbf{x}$ and a latent variable $\\mathbf{t}$ is defined as the product of the conditional distribution over $\\mathbf{x}$ given $\\mathbf{t}$ and the prior distribution over $\\mathbf{t}$:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}, \\mathbf{t} \\lvert \\boldsymbol{\\theta}) = p(\\mathbf{x} \\lvert \\mathbf{t}, \\boldsymbol{\\theta}) p(\\mathbf{t} \\lvert \\boldsymbol{\\theta}).\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "We obtain the [marginal distribution](https://en.wikipedia.org/wiki/Marginal_distribution) over $\\mathbf{x}$ by integrating over $\\mathbf{t}$:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\lvert \\boldsymbol{\\theta}) = \\int p(\\mathbf{x} \\lvert \\mathbf{t}, \\boldsymbol{\\theta}) p(\\mathbf{t} \\lvert \\boldsymbol{\\theta}) d\\mathbf{t}.\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "This integral is usually intractable for even moderately complex conditional probabilities $p(\\mathbf{x} \\lvert \\mathbf{t}, \\boldsymbol{\\theta})$, so is the true posterior:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{t} \\lvert \\mathbf{x}, \\boldsymbol{\\theta}) = {{p(\\mathbf{x} \\lvert \\mathbf{t}, \\boldsymbol{\\theta}) p(\\mathbf{t} \\lvert \\boldsymbol{\\theta})} \\over {p(\\mathbf{x} \\lvert \\boldsymbol{\\theta})}}.\n",
    "\\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Stochastic variational inference\n",
    "\n",
    "The field of mathematics that covers the optimization of a functional w.r.t. a function, like ${\\mathrm{argmax}}_q \\mathcal{L}(\\boldsymbol{\\theta}, q)$, is the [calculus of variations](https://en.wikipedia.org/wiki/Calculus_of_variations), hence the name *variational inference*. In this context, $q$ is called a *variational distribution* and $\\mathcal{L}(\\boldsymbol{\\theta}, q)$ a *variational lower bound*. \n",
    "\n",
    "We will **approximate** the true posterior with a parametric variational distribution $q(\\mathbf{t} \\lvert \\mathbf{x}, \\boldsymbol{\\phi})$ and try to find a value of $\\boldsymbol{\\phi}$ that minimizes the [Kullback–Leibler (KL) divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
    " between this distribution and the true posterior. Using $q(\\mathbf{t} \\lvert \\mathbf{x}, \\boldsymbol{\\phi})$, we can formulate the variational lower bound (explained in [a related notebook](https://nbviewer.jupyter.org/github/krasserm/bayesian-machine-learning/blob/dev/latent-variable-models/latent_variable_models_part_1.ipynb)) for a single observation $\\mathbf{x}_i$ as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\boldsymbol{\\theta}, q; \\mathbf{x}_i) &=\n",
    "\\log p(\\mathbf{x}_i \\lvert \\boldsymbol{\\theta}) - \\mathrm{KL}(q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi}) \\mid\\mid p(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\theta})) \\\\ &= \n",
    "\\log p(\\mathbf{x}_i \\lvert \\boldsymbol{\\theta}) - \\int q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi}) \\log {{q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi})} \\over {p(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\theta})}} d\\mathbf{t}_i \\\\ &= \n",
    "\\int q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi}) \\log {{p(\\mathbf{x}_i \\lvert \\boldsymbol{\\theta}) p(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\theta})} \\over {q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi})}} d\\mathbf{t}_i \\\\ &= \n",
    "\\int q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi}) \\log {{p(\\mathbf{x}_i \\lvert \\mathbf{t}_i, \\boldsymbol{\\theta}) p(\\mathbf{t}_i \\lvert \\boldsymbol{\\theta})} \\over {q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi})}} d\\mathbf{t}_i \\\\ &=\n",
    "\\int q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi}) \\log p(\\mathbf{x}_i \\lvert \\mathbf{t}_i, \\boldsymbol{\\theta}) d\\mathbf{t}_i - \\mathrm{KL}(q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi}) \\mid\\mid p(\\mathbf{t}_i \\lvert \\boldsymbol{\\theta})) \\\\ &=\n",
    "\\mathbb{E}_{q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi})} \\log p(\\mathbf{x}_i \\lvert \\mathbf{t}_i, \\boldsymbol{\\theta}) - \\mathrm{KL}(q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi}) \\mid\\mid p(\\mathbf{t}_i \\lvert \\boldsymbol{\\theta})) \n",
    "\\end{align*}\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "We assume that the integral $\\int q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi}) \\log p(\\mathbf{x}_i \\lvert \\mathbf{t}_i, \\boldsymbol{\\theta}) d\\mathbf{t}_i$ is intractable but we can choose a functional form of $q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi})$ from which we can easily **sample** so that the expectation of $\\log p(\\mathbf{x}_i \\lvert \\mathbf{t}_i, \\boldsymbol{\\theta})$ w.r.t. to $q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi})$ can be **approximated** with $L$ samples from $q$. \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{\\theta}, q; \\mathbf{x}_i) \\approx {1 \\over L} \\sum_{l=1}^L \\log p(\\mathbf{x}_i \\lvert \\mathbf{t}_{i,l}, \\boldsymbol{\\theta}) - \\mathrm{KL}(q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi}) \\mid\\mid p(\\mathbf{t}_i \\lvert \\boldsymbol{\\theta}))\n",
    "\\tag{6}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{t}_{i,l} \\sim q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi})$. We will also choose the functional form of $q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi})$ and $p(\\mathbf{t}_i \\lvert \\boldsymbol{\\theta})$ such that integration of the KL divergence can be done analytically, hence, no samples are needed to evaluate the KL divergence. With these choices, an approximate evaluation of the variational lower bound is possible. But in order to optimize the lower bound w.r.t. $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\phi}$ we need to **approximate** the gradients w.r.t. these parameters. The gradient approximation will not be covered in this notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational autoencoder\n",
    "\n",
    "From the perspective of a generative model, $q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi})$ is a probabilistic *encoder* because it generates a *latent code* $\\mathbf{t}_i$ for input image $\\mathbf{x}_i$ and $p(\\mathbf{x}_i \\lvert \\mathbf{t}_i, \\boldsymbol{\\theta})$ is a probabilistic *decoder* because it generates or reconstructs an image $\\mathbf{x}_i$ from latent code $\\mathbf{t}_i$. Optimizing the variational lower bound w.r.t. parameters $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\phi}$ can therefore be regarded as training a probabilistic autoencoder or *variational autoencoder* (VAE)<sup>[1]</sup>.\n",
    "\n",
    "In this context, the first term on the RHS of Eq. $(5)$ can be interpreted as expected negative *reconstruction error*. The second term is a *regularization term* that encourages the variational distribution to be close to the prior over latent variables. If the regularization term is omitted, the variational distribution would collapse to a delta function and the variational autoencoder would degenerate to a \"usual\" deterministic autoencoder. \n",
    "\n",
    "### Implementation\n",
    "\n",
    "For implementing a variational autoencoder, we make the following choices:\n",
    "\n",
    "- The variational distribution $q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi})$ is a multivariate Gaussian $\\mathcal{N}(\\mathbf{t}_i \\lvert \\boldsymbol\\mu(\\mathbf{x}_i, \\boldsymbol{\\phi}), \\boldsymbol\\sigma^2(\\mathbf{x}_i, \\boldsymbol{\\phi}))$  with a diagonal covariance matrix where mean vector $\\boldsymbol\\mu$ and the covariance diagonal $\\boldsymbol\\sigma^2$ are functions of $\\mathbf{x}_i$ and $\\boldsymbol{\\phi}$. These functions are implemented as neural network and learned during optimization of the lower bound w.r.t. $\\boldsymbol{\\phi}$. After reparameterization, samples from $q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi})$ are obtained via the deterministic function $g(\\boldsymbol\\epsilon, \\mathbf{x}_i, \\boldsymbol{\\phi}) = \\boldsymbol\\mu(\\mathbf{x}_i, \\boldsymbol{\\phi}) + \\boldsymbol\\sigma^2(\\mathbf{x}_i, \\boldsymbol{\\phi}) \\odot \\boldsymbol\\epsilon$ and an auxiliary distribution $p(\\boldsymbol\\epsilon) = \\mathcal{N}(\\boldsymbol\\epsilon \\lvert \\mathbf{0}, \\mathbf{I})$.\n",
    "\n",
    "- The conditional distribution $p(\\mathbf{x}_i \\lvert \\mathbf{t}_i, \\boldsymbol{\\theta})$ is a multivariate Bernoulli distribution $\\text{Ber}(\\mathbf{x}_i \\lvert \\mathbf{k}(\\mathbf{t}_i, \\boldsymbol{\\theta}))$ where parameter $\\mathbf{k}$ is a function of $\\mathbf{t}_i$ and $\\boldsymbol{\\theta}$. This distribution models the binary training data i.e. monochrome (= binarized) MNIST images in our example. Function $\\mathbf{k}$ computes for each pixel its expected value. It is also implemented as neural network and learned during optimization of the lower bound w.r.t. $\\boldsymbol{\\theta}$. Taking the (negative) logarithm of $\\text{Ber}(\\mathbf{x}_i \\lvert \\mathbf{k}(\\mathbf{t}_i, \\boldsymbol{\\theta}))$ gives a sum over pixel-wise binary cross entropies as shown in Eq. $(12)$\n",
    "\n",
    "- Prior $p(\\mathbf{t}_i \\lvert \\boldsymbol{\\theta})$ is a multivariate Gaussian distribution $\\mathcal{N}(\\mathbf{t}_i \\lvert \\mathbf{0}, \\mathbf{I})$ with zero mean and unit covariance matrix. With the chosen functional forms of the prior and the variational distribution $q$, $\\mathrm{KL}(q(\\mathbf{t}_i \\lvert \\mathbf{x}_i, \\boldsymbol{\\phi}) \\mid\\mid p(\\mathbf{t}_i \\lvert \\boldsymbol{\\theta}))$ can be integrated analytically to $-{1 \\over 2} \\sum_{d=1}^D (1 + \\log \\sigma_{i,d}^2 - \\mu_{i,d}^2 - \\sigma_{i,d}^2)$ where $D$ is the dimensionality of the latent space and $\\mu_{i,d}$ and $\\sigma_{i,d}$ is the $d$-th element of $\\boldsymbol\\mu(\\mathbf{x}_i, \\boldsymbol{\\phi})$ and $\\boldsymbol\\sigma(\\mathbf{x}_i, \\boldsymbol{\\phi})$, respectively.\n",
    "\n",
    "Using these choices and setting $L = 1$, the variational lower bound for a single image $\\mathbf{x}_i$ can be approximated as\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{\\theta}, q; \\mathbf{x}_i) \\approx \n",
    "- \\sum_c \\left(x_{i,c} \\log k_{i,c} + (1 - x_{i,c}) \\log (1 - k_{i,c})\\right) + {1 \\over 2} \\sum_d (1 + \\log \\sigma_{i,d}^2 - \\mu_{i,d}^2 - \\sigma_{i,d}^2)\n",
    "\\tag{12}\n",
    "$$\n",
    "\n",
    "where $x_{i,c}$ is the value of pixel $c$ in image $\\mathbf{x}_i$ and $k_{i,c}$ its expected value. The negative value of the lower bound is used as loss during training. The following figure outlines the architecture of the variational autoencoder. \n",
    "\n",
    "![VAE](images/vae/auto-encoder.jpg)\n",
    "\n",
    "The definitions of the encoder and decoder neural networks were taken from \\[2\\]. Here, the encoder computes the logarithm of the variance, instead of the variance directly, for reasons of numerical stability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE MNIST Example\n",
    "batch_size=128 # input batch size for training (default: 128)\n",
    "Max_epochs=10 # number of epochs to train (default: 10)')\n",
    "\n",
    "log_interval=1000 # how many batches to wait before logging training status\n",
    "\n",
    "torch.manual_seed(2020)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 117.837433\n",
      "====> Epoch: 1 Average loss: 116.7120\n",
      "====> Test set loss: 113.0776\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 113.539566\n",
      "====> Epoch: 2 Average loss: 112.6641\n",
      "====> Test set loss: 110.6645\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 110.041824\n",
      "====> Epoch: 3 Average loss: 110.5308\n",
      "====> Test set loss: 109.1412\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 107.372475\n",
      "====> Epoch: 4 Average loss: 109.1225\n",
      "====> Test set loss: 107.8147\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 103.284172\n",
      "====> Epoch: 5 Average loss: 108.1746\n",
      "====> Test set loss: 107.0832\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 110.811150\n",
      "====> Epoch: 6 Average loss: 107.4493\n",
      "====> Test set loss: 106.4892\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 109.923370\n",
      "====> Epoch: 7 Average loss: 106.8450\n",
      "====> Test set loss: 106.2193\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 107.901100\n",
      "====> Epoch: 8 Average loss: 106.4145\n",
      "====> Test set loss: 105.8292\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 105.954086\n",
      "====> Epoch: 9 Average loss: 106.0637\n",
      "====> Test set loss: 105.5680\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 105.647720\n",
      "====> Epoch: 10 Average loss: 105.7093\n",
      "====> Test set loss: 105.3276\n"
     ]
    }
   ],
   "source": [
    "    for epoch in range(1, Max_epochs + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Compare the improvement in convergence speed in this version using ReLu and adam vs the original verison using sigmoid and adagrad\n",
    "\n",
    "Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The variational autoencoder (VAE) is arguably the simplest setup that realizes deep probabilistic modeling. Note that we're being careful in our choice of language here. The VAE isn't a model as such&mdash;rather the VAE is a particular setup for doing variational inference for a certain class of models. The class of models is quite broad: basically\n",
    "any (unsupervised) density estimator with latent random variables. The basic structure of such a model is simple, almost deceptively so (see Fig. 1)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<center><figure><img src=\"_static/img/vae_model.png\" style=\"width: 200px;\"><figcaption> <font size=\"+1\"><b>Figure 1</b>: the class of deep models we're interested in.</font></figcaption></figure></center><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we've depicted the structure of the kind of model we're interested in as a graphical model. We have $N$ observed datapoints $\\{ \\bf x_i \\}$. Each datapoint is generated by a (local) latent random variable $\\bf z_i$. There is also a parameter $\\theta$, which is global in the sense that all the datapoints depend on it (which is why it's drawn outside the rectangle). Note that since $\\theta$ is a parameter, it's not something we're being Bayesian about. Finally, what's of particular importance here is that we allow for each $\\bf x_i$ to depend on $\\bf z_i$ in a complex, non-linear way. In practice this dependency will be parameterized by a (deep) neural network with parameters $\\theta$. It's this non-linearity that makes inference for this class of models particularly challenging. \n",
    "\n",
    "Of course this non-linear structure is also one reason why this class of models offers a very flexible approach to modeling complex data. Indeed it's worth emphasizing that each of the components of the model can be 'reconfigured' in a variety of different ways. For example:\n",
    "\n",
    "- the neural network in $p_\\theta({\\bf x} | {\\bf z})$ can be varied in all the usual ways (number of layers, type of non-linearities, number of hidden units, etc.)\n",
    "- we can choose observation likelihoods that suit the dataset at hand: gaussian, bernoulli, categorical, etc.\n",
    "- we can choose the number of dimensions in the latent space\n",
    "\n",
    "The graphical model representation is a useful way to think about the structure of the model, but it can also be fruitful to look at an explicit factorization of the joint probability density:\n",
    "\n",
    "$$ p({\\bf x}, {\\bf z}) = \\prod_{i=1}^N p_\\theta({\\bf x}_i | {\\bf z}_i) p({\\bf z}_i)  $$\n",
    "\n",
    "The fact that $p({\\bf x}, {\\bf z})$ breaks up into a product of terms like this makes it clear what we mean when we call $\\bf z_i$ a local random variable. For any particular $i$, only the single datapoint $\\bf x_i$ depends on $\\bf z_i$. As such the $\\{\\bf z_i\\}$ describe local structure, i.e. structure that is private to each data point. This factorized structure also means that we can do subsampling during the course of learning. As such this sort of model is amenable to the large data setting. (For more discussion on this and related topics see [SVI Part II](svi_part_ii.ipynb).)\n",
    "\n",
    "That's all there is to the model. Since the observations depend on the latent random variables in a complicated, non-linear way, we expect the posterior over the latents to have a complex structure. Consequently in order to do inference in this model we need to specify a flexibly family of guides (i.e. variational distributions). Since we want to be able to scale to large datasets, our guide is going to make use of amortization to keep the number of variational parameters under control (see [SVI Part II](svi_part_ii.ipynb) for a somewhat more general discussion of amortization). \n",
    "\n",
    "Recall that the job of the guide is to 'guess' good values for the latent random variables&mdash;good in the sense that they're true to the model prior _and_ true to the data. If we weren't making use of amortization, we would introduce variational parameters \n",
    "$\\{ \\lambda_i \\}$ for _each_ datapoint $\\bf x_i$. These variational parameters would represent our belief about 'good' values of $\\bf z_i$; for example, they could encode the mean and variance of a gaussian distribution in ${\\bf z}_i$ space. Amortization means that, rather than introducing variational parameters $\\{ \\lambda_i \\}$, we instead learn a _function_ that maps each $\\bf x_i$ to an appropriate $\\lambda_i$. Since we need this function to be flexible, we parameterize it as a neural network. We thus end up with a parameterized family of distributions over the latent $\\bf z$ space that can be instantiated for all $N$ datapoint ${\\bf x}_i$ (see Fig. 2)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<center><figure><img src=\"_static/img/vae_guide.png\" style=\"width: 200px;\"><figcaption> <font size=\"+1\"><b>Figure 2</b>: a graphical representation of the guide. </font></figcaption></figure></center><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the guide $q_{\\phi}({\\bf z} | {\\bf x})$ is parameterized by a global parameter $\\phi$ shared by all the datapoints. The goal of inference will be to find 'good' values for $\\theta$ and $\\phi$ so that two conditions are satisfied:\n",
    "\n",
    "- the log evidence $\\log p_\\theta({\\bf x})$ is large. this means our model is a good fit to the data\n",
    "- the guide $q_{\\phi}({\\bf z} | {\\bf x})$ provides a good approximation to the posterior \n",
    "\n",
    "(For an introduction to stochastic variational inference see [SVI Part I](svi_part_i.ipynb).)\n",
    "\n",
    "At this point we can zoom out and consider the high level structure of our setup. For concreteness, let's suppose the $\\{ \\bf x_i \\}$ are images so that the model is a generative model of images. Once we've learned a good value of $\\theta$ we can generate images from the model as follows:\n",
    "\n",
    "- sample $\\bf z$ according to the prior $p({\\bf z})$\n",
    "- sample $\\bf x$ according to the likelihood $p_\\theta({\\bf x}|{\\bf z})$\n",
    "\n",
    "Each image is being represented by a latent code $\\bf z$ and that code gets mapped to images using the likelihood, which depends on the $\\theta$ we've learned. This is why the likelihood is often called the decoder in this context: its job is to decode $\\bf z$ into $\\bf x$. Note that since this is a probabilistic model, there is uncertainty about the $\\bf z$ that encodes a given datapoint $\\bf x$.\n",
    "\n",
    "Once we've learned good values for $\\theta$ and $\\phi$ we can also go through the following exercise. \n",
    "\n",
    "- we start with a given image $\\bf x$\n",
    "- using our guide we encode it as $\\bf z$\n",
    "- using the model likelihood we decode $\\bf z$ and get a reconstructed image ${\\bf x}_\\rm{reco}$\n",
    "\n",
    "If we've learned good values for $\\theta$ and $\\phi$, $\\bf x$ and ${\\bf x}_\\rm{reco}$ should be similar. This should clarify how the word autoencoder ended up being used to describe this setup: the model is the decoder and the guide is the encoder. Together, they can be thought of as an autoencoder.\n",
    "\n",
    "## VAE in Pyro\n",
    "\n",
    "Let's see how we implement a VAE in Pyro.\n",
    "The dataset we're going to model is MNIST, a collection of images of handwritten digits.\n",
    "Since this is a popular benchmark dataset, we can make use of PyTorch's convenient data loader functionalities to reduce the amount of boilerplate code we need to write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.examples.util  # patches torchvision\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)\n",
    "pyro.set_rng_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and batching MNIST dataset\n",
    "batch_size=128\n",
    "mnist_train = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST('data', train=False, download=True, transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main thing to draw attention to here is that we use `transforms.ToTensor()` to normalize the pixel intensities to the range $[0.0, 1.0]$. \n",
    "\n",
    "Next we define a PyTorch module that encapsulates our decoder network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a latent code $z$, the forward call of `Decoder`  returns the parameters for a Bernoulli distribution in image space. Since  each image is of size\n",
    "$28\\times28=784$, `loc_img` is of size `batch_size` x 784.\n",
    "\n",
    "Next we define a PyTorch module that encapsulates our encoder network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an image $\\bf x$ the forward call of `Encoder` returns a mean and covariance that together parameterize a (diagonal) Gaussian distribution in latent space.\n",
    "\n",
    "With our encoder and decoder networks in hand, we can now write down the stochastic functions that represent our model and guide. First the model: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `model()` is a callable that takes in a mini-batch of images `x` as input. This is a `torch.Tensor` of size `batch_size` x 784.\n",
    "\n",
    "The first thing we do inside of `model()` is register the (previously instantiated) decoder module with Pyro. Note that we give it an appropriate (and unique) name. This call to `pyro.module` lets Pyro know about all the parameters inside of the decoder network. \n",
    "\n",
    "Next we setup the hyperparameters for our prior, which is just a unit normal gaussian distribution. Note that:\n",
    "- we specifically designate independence amongst the data in our mini-batch (i.e. the leftmost dimension) via `pyro.plate`. Also, note the use of `.to_event(1)` when sampling from the latent `z` - this ensures that instead of treating our sample as being generated from a univariate normal with `batch_size = z_dim`, we treat them as being generated from a multivariate normal distribution with diagonal covariance. As such, the log probabilities along each dimension is summed out when we evaluate `.log_prob` for a \"latent\" sample. Refer to the [Tensor Shapes](tensor_shapes.ipynb) tutorial for more details.\n",
    "- since we're processing an entire mini-batch of images, we need the leftmost dimension of `z_loc` and `z_scale` to equal the mini-batch size\n",
    "- in case we're on GPU, we use `new_zeros` and `new_ones` to ensure that newly created tensors are on the same GPU device.\n",
    "\n",
    "Next we sample the latent `z` from the prior, making sure to give the random variable a unique Pyro name `'latent'`. \n",
    "Then we pass `z` through the decoder network, which returns `loc_img`. We then score the observed images in the mini-batch `x` against the Bernoulli likelihood parametrized by `loc_img`.\n",
    "Note that we flatten `x` so that all the pixels are in the rightmost dimension.\n",
    "\n",
    "That's all there is to it! Note how closely the flow of Pyro primitives in `model` follows the generative story of our model, e.g. as encapsulated by Figure 1. Now we move on to the guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the guide (i.e. variational distribution) q(z|x)\n",
    "def guide(self, x):\n",
    "    # register PyTorch module `encoder` with Pyro\n",
    "    pyro.module(\"encoder\", self.encoder)\n",
    "    with pyro.plate(\"data\", x.shape[0]):\n",
    "        # use the encoder to get the parameters used to define q(z|x)\n",
    "        z_loc, z_scale = self.encoder(x)\n",
    "        # sample the latent code z\n",
    "        pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the model, we first register the PyTorch module we're using (namely `encoder`) with Pyro. We take the mini-batch of images `x` and pass it through the encoder. Then using the parameters output by the encoder network we use the normal distribution to sample a value of the latent for each image in the mini-batch. Crucially, we use the same name for the latent random variable as we did in the model: `'latent'`. Also, note the use of `pyro.plate` to designate independence of the mini-batch dimension, and `.to_event(1)` to enforce dependence on `z_dims`, exactly as we did in the model.\n",
    "\n",
    "Now that we've defined the full model and guide we can move on to inference. But before we do so let's see how we package the model and guide in a PyTorch module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, z_dim=50, hidden_dim=400, use_cuda=False):\n",
    "        super().__init__()\n",
    "        # create the encoder and decoder networks\n",
    "        self.encoder = Encoder(z_dim, hidden_dim)\n",
    "        self.decoder = Decoder(z_dim, hidden_dim)\n",
    "\n",
    "        if use_cuda:\n",
    "            # calling cuda() here will put all the parameters of\n",
    "            # the encoder and decoder networks into gpu memory\n",
    "            self.cuda()\n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    # define the model p(x|z)p(z)\n",
    "    def model(self, x):\n",
    "        # register PyTorch module `decoder` with Pyro\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # setup hyperparameters for prior p(z)\n",
    "            z_loc = x.new_zeros(torch.Size((x.shape[0], self.z_dim)))\n",
    "            z_scale = x.new_ones(torch.Size((x.shape[0], self.z_dim)))\n",
    "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
    "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "            # decode the latent code z\n",
    "            loc_img = self.decoder(z)\n",
    "            # score against actual images\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(loc_img).to_event(1), obs=x.reshape(-1, 784))\n",
    "\n",
    "    # define the guide (i.e. variational distribution) q(z|x)\n",
    "    def guide(self, x):\n",
    "        # register PyTorch module `encoder` with Pyro\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # use the encoder to get the parameters used to define q(z|x)\n",
    "            z_loc, z_scale = self.encoder(x)\n",
    "            # sample the latent code z\n",
    "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "\n",
    "    # define a helper function for reconstructing images\n",
    "    def reconstruct_img(self, x):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.encoder(x)\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        loc_img = self.decoder(z)\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point we'd like to make here is that the two `Module`s `encoder` and `decoder` are attributes of `VAE` (which itself inherits from `nn.Module`). This has the consequence they are both automatically registered as belonging to the `VAE` module. So, for example, when we call `parameters()` on an instance of `VAE`, PyTorch will know to return all the relevant parameters. It also means that if we're running on a GPU, the call to `cuda()` will move all the parameters of all the (sub)modules into GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We're now ready for inference. Refer to the full code in the next section. \n",
    "\n",
    "First we instantiate an instance of the `VAE` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we setup an instance of the Adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we setup our inference algorithm, which is going to learn good parameters for the model and guide by maximizing the ELBO:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi = SVI(myVAE.model, myVAE.guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all there is to it. Now we just have to define our training loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all the mini-batch logic is handled by the data loader. The meat of the training loop is `svi.step(x)`. There are two things we should draw attention to here:\n",
    "\n",
    "- any arguments to `step` are passed to the model and the guide. consequently `model` and `guide` need to have the same call signature\n",
    "- `step` returns a noisy estimate of the loss (i.e. minus the ELBO). this estimate is not normalized in any way, so e.g. it scales with the size of the mini-batch\n",
    "\n",
    "The logic for adding evaluation logic is analogous:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the only change we need to make is that we call evaluate_loss instead of step. This function will compute an estimate of the ELBO but won't take any gradient steps.\n",
    "\n",
    "The final piece of code we'd like to highlight is the helper method `reconstruct_img` in the VAE class: This is just the image reconstruction experiment we described in the introduction translated into code. We take an image and pass it through the encoder. Then we sample in latent space using the gaussian distribution provided by the encoder. Finally we decode the latent code into an image: we return the mean vector `loc_img` instead of sampling with it. Note that since the `sample()` statement is stochastic, we'll get different draws of z every time we run the reconstruct_img function. If we've learned a good model and guide—in particular if we've learned a good latent representation—this plurality of z samples will correspond to different styles of digit writing, and the reconstructed images should exhibit an interesting variety of different styles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code and Sample results\n",
    "\n",
    "Training corresponds to maximizing the evidence lower bound (ELBO) over the training dataset. We train for 100 iterations and evaluate the ELBO for the test dataset, see Figure 3."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<center>\n",
    "<figure>\n",
    "<img src=\"_static/img/vae_plots/test_elbo_vae.png\"  style=\"width: 550px;\">\n",
    "<figcaption>\n",
    "<font size=\"+1\"><b>Figure 3:</b> How the test ELBO evolves over the course of training. </font>\n",
    "</figcaption>\n",
    "</figure>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we show a set of randomly sampled images from the model. These are generated by drawing random samples of `z` and generating an image for each one, see Figure 4."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<center>\n",
    "<figure>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <td> \n",
    "                <img src=\"_static/img/vae_plots/vae_embeddings_pt1.jpg\"  style=\"width: 350px;\">\n",
    "            </td>\n",
    "            <td> \n",
    "                <img src=\"_static/img/vae_plots/vae_embeddings_pt2.jpg\" style=\"width: 350px;\">\n",
    "            </td>\n",
    "        </tr>\n",
    "    </table> \n",
    "    <figcaption> \n",
    "        <font size=\"+1\"><b>Figure 4:</b> Samples from generative model.</font>\n",
    "    </figcaption>\n",
    "</figure>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also study the 50-dimensional latent space of the entire test dataset by encoding all MNIST images and embedding their means into a 2-dimensional T-SNE space. We then color each embedded image by its class.\n",
    "The resulting Figure 5 shows separation by class with variance within each class-cluster."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<center>\n",
    "<figure>\n",
    "<img src=\"_static/img/vae_plots/VAE_embedding.png\"  style=\"width: 550px;\">\n",
    "<figcaption>\n",
    "<font size=\"+1\"><b>Figure 5:</b> T-SNE Embedding of the latent z. The colors correspond to different classes of digits.</font>\n",
    "</figcaption>\n",
    "</figure>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] `Auto-Encoding Variational Bayes`,<br/>&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Diederik P Kingma, Max Welling\n",
    "\n",
    "[2] `Stochastic Backpropagation and Approximate Inference in Deep Generative Models`,\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercises\n",
    "\n",
    "\n",
    "* Explore other VAEs following the [PyTorch-VAE](https://github.com/AntixK/PyTorch-VAE) repository to generate new faces from celebrities.\n",
    "* Explore [PyroLab 1 - Bayesian Linear Regression for Generative Learning with Pyro](https://github.com/haipinglu/SimplyDeep/blob/master/PyroLab%201%20-%20Bayesian%20Linear%20Regression%20for%20Generative%20Learning%20with%20Pyro.ipynb)\n",
    "* Explore [PyroLab 2 - Variational Autoencoder for Deep Generative Learning](https://github.com/haipinglu/SimplyDeep/blob/master/PyroLab%202%20-%20Variational%20Autoencoder%20for%20Deep%20Generative%20Learning.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
