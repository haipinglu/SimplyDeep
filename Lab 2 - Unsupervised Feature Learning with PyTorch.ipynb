{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Unsupervised Feature Learning with PyTorch\n",
    "\n",
    "## Objective\n",
    "\n",
    "* To perform unsupervised feature learning via autoencoder in PyTorch.\n",
    "\n",
    "**Suggested reading**: \n",
    "* [Autoencoder - Wikipedia](https://en.wikipedia.org/wiki/Autoencoder)\n",
    "* [Autograd tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why\n",
    "\n",
    "[Unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning) is a fundamental problem in machine learning where we aim to learn from unlabelled data. Unsupervised feature learning or representation learning can find wide usage in various applications for extracting useful information from often abundant unlabelled data.\n",
    "\n",
    "## 1. Autograd: Automatic Differentiation\n",
    "\n",
    "In the previous lab, we briefly covered **Tensor** and **Computational Graph**. We have actually used **Autograd** already. Here, we learn the basics below, a condensed and modified version of the original [PyTorch tutorial on Autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)\n",
    "\n",
    "#### Why differentiation is important? \n",
    "\n",
    "This is because it is a key procedure in **optimisation** to find the optimial solution of a loss function. The process of learning/training aims to minimise a predefined loss.\n",
    "\n",
    "#### How automatic differentiation is done in pytorch?\n",
    "The PyTorch ``autograd`` package makes differentiation (almost) transparent to you by providing automatic differentiation for all operations on Tensors, unless you donot want it (to save time and space). \n",
    "\n",
    "A ``torch.Tensor`` type variable has an attribute ``.requires_grad``. Setting this attribute ``True`` tracks (but not computes yet) all operations on it. After we define the forward pass, and hence the *computational graph*, we call ``.backward()`` and all the gradients will be computed automatically and accumulated into the ``.grad`` attribute. \n",
    "\n",
    "This is made possible by the [**chain rule of differentiation**](https://en.wikipedia.org/wiki/Chain_rule).\n",
    "\n",
    "#### How to stop automatic differentiation (e.g., because it is not needed)\n",
    "Calling method ``.detach()`` of a tensor will detach it from the computation history. We can also wrap the code block in ``with torch.no_grad():`` so all tensors in the block do not track the gradients, e.g., in the test/evaluation stage. \n",
    "\n",
    "### Function\n",
    "\n",
    "``Tensor``s are connected by ``Function`` to build an acyclic *computational graph* to encode a complete history of computation. The ``.grad_fn`` attribute of a tensor references a ``Function`` created\n",
    "the ``Tensor``, i.e., this ``Tensor`` is the output of its ``.grad_fn`` in the computational graph.\n",
    "\n",
    "Learn more about autograd by referring to the [documentation on autograd](https://pytorch.org/docs/stable/autograd.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building and Optimising an Autoencoder in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus on *computer vision* applications. We are going to build an autoencoder to learn a low-dimensional representation of some specific images, in a particular dataset. We just proceed and explain as we go. This part follows the [Autoencoder notebook by Lisa Zhang](https://www.cs.toronto.edu/~lczhang/aps360_20191/lec/w05/autoencoder.html) with additional explanation and minor changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libaries\n",
    "\n",
    "Get ready by importing the standard APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "Let us work with the popular [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) with handwritten digits. We will work on a subset for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
    "print(len(mnist_data))\n",
    "mnist_data = list(mnist_data)[:4096]\n",
    "print(len(mnist_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the NN architecture\n",
    "In Lab 1, we did not define a class for our linear regression NN. Here we do so and define an autoencoder class consisting of an **encoder** followed by a **decoder** as defined below.\n",
    "<img src=\"https://miro.medium.com/max/3524/1*oUbsOnYKX5DEpMOK3pH_lg.png\" style=\"width:360px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 1 input image channel, 16 output channel, 3x3 square convolution\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 7)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  #to range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__init__()` defines the layers.  `forward()` defines the *forward pass* that transform the input to the output. `backward()` is automatically defined using `autograd`.\n",
    "\n",
    "Here, we have both convolution layers `Conv2d()` and transpose convolution layers `ConvTranspose2d()`, with nice illustrations at [Convolution arithmetic](https://github.com/vdumoulin/conv_arithmetic). The basic ones are reproduced below where blue maps indicate inputs, and cyan maps indicate outputs.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "    <td  style=\"text-align: left\"> Convolution with no padding, no strides.      <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "    <td  style=\"text-align: left\"> Transpose convolution with No padding, no strides.<img src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_no_strides_transposed.gif\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "`ReLu()` and `Sigmoid()` are [rectified linear unit (ReLU)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) and [Sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function), two popular **activation function** that performs a *nonlinear* transformation/mapping of an input variable (element-wise operation).\n",
    "\n",
    "\n",
    "#### Inspect the NN architecture\n",
    "\n",
    "Now let's take a look at the autoencoder built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myAE=Autoencoder()\n",
    "print(myAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the (randomly initialised) parameters of this NN. Below, we check the first 2D convolution and the ReLu activiation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = list(myAE.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # First Conv2d's .weight\n",
    "print(params[1].size())  # First Conv2d's .bias\n",
    "print(params[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about these functions, refer to the [`torch.nn` documentation](https://pytorch.org/docs/stable/nn.html) (search for the function, e.g., search for `torch.nn.ReLu(` and you will find its documentation [here](https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.ReLU)\n",
    "\n",
    "#### Train the NN\n",
    "Next, we will feed data in this autoencoder to train it, i.e., learn its parameters so that the reconstruction error (the `loss`) is minimised, using the mean square error (MSE) and `Adam` optimiser. The dataset is loaded in batches to train the model. One `epoch` means one cycle through the full training dataset. The `outputs` at the end of each epoch save the orignal image and the reconstructed (decoded) image pairs for later inspection. The steps are \n",
    "* Define the optimisation criteria and optimisation method.\n",
    "* Iterate through the whole dataset in batches, for a number of `epochs` till a maximum specified or a convergence criteria (e.g., successive change of loss < 0.000001)\n",
    "* In each batch processing, we \n",
    "    * do a forward pass\n",
    "    * compute the loss\n",
    "    * backpropagate the loss via `autograd`\n",
    "    * update the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Hyperparameters for training\n",
    "batch_size=64\n",
    "learning_rate=1e-3\n",
    "max_epochs = 20\n",
    "\n",
    "#Set the random seed for reproducibility \n",
    "torch.manual_seed(509) \n",
    "#Choose mean square error loss\n",
    "criterion = nn.MSELoss() \n",
    "#Choose the Adam optimiser\n",
    "optimizer = torch.optim.Adam(myAE.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "#Specify how the data will be loaded in batches (with random shffling)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_data, batch_size=batch_size, shuffle=True)\n",
    "#Storage\n",
    "outputs = []\n",
    "\n",
    "#Start training\n",
    "for epoch in range(max_epochs):\n",
    "    for data in train_loader:\n",
    "        img, label = data\n",
    "        optimizer.zero_grad()\n",
    "        recon = myAE(img)\n",
    "        loss = criterion(recon, img)\n",
    "        loss.backward()\n",
    "        optimizer.step()            \n",
    "    if (epoch % 3) == 0:\n",
    "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))\n",
    "    outputs.append((epoch, img, recon),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at how `autograd` keeps track of the gradients for back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.grad_fn)\n",
    "print(loss.grad_fn.next_functions[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the loss is being well minimised. We can inspect the reconstructed images at different epochs below. Note here we use `.detach()` because the gradients are not needed for inspection purpose here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numImgs=12;\n",
    "for k in range(0, max_epochs, 9):\n",
    "    plt.figure(figsize=(numImgs, 2))\n",
    "    imgs = outputs[k][1].detach().numpy()    \n",
    "    recon = outputs[k][2].detach().numpy()\n",
    "    for i, item in enumerate(imgs):\n",
    "        if i >= numImgs: break\n",
    "        plt.subplot(2, numImgs, i+1)\n",
    "        plt.imshow(item[0])\n",
    "        \n",
    "    for i, item in enumerate(recon):\n",
    "        if i >= numImgs: break\n",
    "        plt.subplot(2, numImgs, numImgs+i+1)\n",
    "        plt.imshow(item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate synthesised images\n",
    "We can **interpolate** between two images via the learned embeddding. Let us pick the second `7` and four image `3` from the first epoch and obtain the low-dimensional embedding using the learned encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochIndex=0;\n",
    "img1Index=1;\n",
    "img2Index=3;\n",
    "\n",
    "imgs = outputs[epochIndex][1].detach().numpy()\n",
    "x1 = outputs[epochIndex][1][img1Index,:,:,:];# first image\n",
    "x2 = outputs[epochIndex][1][img2Index,:,:,:] # second image\n",
    "x = torch.stack([x1,x2])     # stack them together so we only call `encoder` once\n",
    "embedding = myAE.encoder(x)\n",
    "e1 = embedding[0] # embedding of first image\n",
    "e2 = embedding[1] # embedding of second image\n",
    "print(e1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the embedding space, we do a linear interpolation between the two embeddings and then decode these interpolated embeddings into images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_values = []\n",
    "for i in range(0, 10):\n",
    "    e = e1 * (i/10) + e2 * (10-i)/10\n",
    "    embedding_values.append(e)\n",
    "embedding_values = torch.stack(embedding_values)\n",
    "\n",
    "recons = myAE.decoder(embedding_values)\n",
    "plt.figure(figsize=(10, 2))\n",
    "for i, recon in enumerate(recons.detach().numpy()):\n",
    "    plt.subplot(2,10,i+1)\n",
    "    plt.imshow(recon[0])\n",
    "plt.subplot(2,10,11)\n",
    "plt.imshow(imgs[img2Index][0])\n",
    "plt.subplot(2,10,20)\n",
    "plt.imshow(imgs[img1Index][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercises\n",
    "\n",
    "\n",
    "* Code PCA using ``torch.nn`` and compare it with the close-form solution via eigendecomposition.\n",
    "* Try out different optimisers or different loss function (the L1loss, MAE) and compare the results.\n",
    "* Change the architecture of autoencoder (e.g., depth, other layers such as max pooling, different activation functions) to compare the results.\n",
    "* Repeat the above on a subset from the CIFAR10 dataset. See example usage [here](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). For example, you can interpolate a cat and a dog.\n",
    "\n",
    "\n",
    "\n",
    "# Acknowledgement\n",
    "Some part of this notebook is adapted from the following sources\n",
    "\n",
    "* [Autoencoder notebook by Lisa Zhang from APS360, University of Toronto](https://www.cs.toronto.edu/~lczhang/aps360_20191/lec/w05/autoencoder.html)\n",
    "\n",
    "# References\n",
    "\n",
    "* [CS231n: Convolutional Neural Networks for Visual Recognition from Stanford](http://cs231n.github.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
